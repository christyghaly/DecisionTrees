{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5\n",
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are Supervised Machine learning Algorithm,They can be used for Classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick Example\n",
    "The following code trains a DecisionTreeClassifier on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install python-pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)\n",
    "\n",
    "export_graphviz( #Export a decision tree in DOT format\n",
    "tree_clf,\n",
    "out_file=\"iris_tree.dot\",\n",
    "feature_names=iris.feature_names[2:],\n",
    "class_names=iris.target_names,\n",
    "rounded=True,\n",
    "filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "\n",
    "(graph,) = pydot.graph_from_dot_file('iris_tree.dot')\n",
    "graph.write_png('iris_tree.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"iris_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  How prediction works:\n",
    "Suppose you find an iris flower and you want to classify it.\n",
    "\n",
    "You start at the root node (depth 0, at the top): this node asks whether the flower’s petal length is smaller than 2.45 cm.\n",
    "1. If it is, then you move down to the root’s left child node (depth 1, left). In this case, it is a leaf node setosa class.\n",
    "2. If it is not (petal length is greater than 2.45 cm) : You must move down to the root’s right child node (depth 1, right) which is not a leaf node.\n",
    "3. It asks anotherquestion: is the petal width smaller than 1.75 cm?\n",
    "4. If it is, Then go to the next level to the left which is a leaf node versicolor class.\n",
    "5. If it is not, Then go to the next level to the right which is a leaf node virginica class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nodes calculations:\n",
    "Each node contain a condition which the question that spearate the data and it have different attributes to calculate such as samples, value, and gini.\n",
    "\n",
    "#### samples\n",
    "A node’s samples attribute counts how many training instances it applies to. For example. the iris flower dataset contains 150 training samples which are counted at the root. Then, 100 samples have petal length greater than 2.45 cm at level 1 right node. Furthermore, the 100 samples contains 54 versicolor class and 46 virginica class as counted in the right leafs.\n",
    "\n",
    "#### values\n",
    "A nodes's Value represents tells you how many training instances of each class this node applies to.for example, the bottom-right node applies to 0 Iris-Setosa, 1 IrisVersicolor, and 45 Iris-Virginica\n",
    "\n",
    "gini\n",
    "a node’s gini attribute measures its impurity: a node is pure (gini=0) if all training instances it applies to belong to the same class. For example, since the depth-1 left node applies only to Iris-Setosa training instances, it is pure and its gini score is 0. The following equation calculates score Gi of the ith node:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ G_{i} = 1 - \\sum_{k=1}^{n} P_{i,k}^{2} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where Pi,kis the ratio of class k instances among the training instances in the ith node.\n",
    "\n",
    "For example, the depth-2 left node has a gini score equal to $$ 1 – {(0/54)}^{2} – {(49/54)}^{2} – {(5/54)}^{2} ≈ 0.168. $$ \n",
    "\n",
    "The following figure shows Decision Tree’s decision boundaries. The thick vertical line represents the decision boundary of the root node (depth 0): petal length = 2.45 cm. Since the left area is pure (only Iris-Setosa), it cannot be split any further. However, the right area is impure, so the depth-1 right node splits it at petal width = 1.75 cm (represented by the dashed line). Since max_depth was set to 2, the Decision Tree stops right there. However, if you set max_depth to 3, then the two depth-2 nodes would each add another decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"irisGraph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Probability Estimation\n",
    "A Decision Tree can also estimate the probability that an instance belongs to a particular class k: first it traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of class k in this node. For example, suppose you have found a flower whose petals are 5 cm long and 1.5 cm wide. The corresponding leaf node is the depth-2 left node, so the Decision Tree should output the following probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54), and 9.3% for IrisVirginica (5/54). And of course if you ask it to predict the class, it should output Iris-Versicolor (class 1) since it has the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The representation for the CART model is a binary tree.It only gives two outputs from the node (True or False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CART_TREE.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Can It gets the Split points and the best Feature ???\n",
    "\n",
    "Creating a CART model involves selecting input variables and split points on those variables until a suitable tree is constructed.\n",
    "\n",
    "The selection of which input variable to use and the specific split or cut-point is chosen using by minimizin a cost function. Tree construction ends using a predefined stopping criterion, such as a minimum number of training instances assigned to each leaf node of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All input variables and all possible split points are evaluated and chosen in a greedy manner (e.g. the very best split point is chosen each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function that is minimized in Classification\n",
    "<img src=\"Cost_fun_class.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function that is minimized in Regression\n",
    "<img src=\"Cost_fun_regression.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "L = 2 * np.random.rand(200, 1)\n",
    "F = 4 + 3 * L + np.random.randn(200, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(L, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz( #Export a decision tree in DOT format\n",
    "tree_reg,\n",
    "out_file=\"regression_tree.dot\",\n",
    "rounded=True,\n",
    "filled=True\n",
    ")\n",
    "import pydot\n",
    "\n",
    "(graph,) = pydot.graph_from_dot_file('regression_tree.dot')\n",
    "graph.write_png('regression_tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='regression_tree.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Here is The prediction which is simply the average target value of the training\n",
    "instances associated to this leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
